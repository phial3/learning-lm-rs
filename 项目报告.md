## 一、作业阶段

### 1. SiLU算子

```rust
// y = silu(x) * y
// hint: this is an element-wise operation
pub fn swiglu<T: Copy + Default + Float + Sum>(y: &mut Tensor<T>, x: &Tensor<T>) {
    let len = y.size();
    assert!(len == x.size());

    let _y = unsafe { y.data_mut() };
    let _x = x.data();

    let _x = _x
        .iter()
        .map(|the_x| *the_x * (T::one() / (T::one() + (-*the_x).exp())))
        .collect::<Vec<T>>();
    let mut idx = 0;
    for the_y in _y.iter_mut() {
        *the_y = *the_y * _x[idx];
        idx += 1;
    }
}
```

### 2. RMSNorm

```rust
pub fn rms_norm<T: Copy + Default + Float + Sum>(
    y: &mut Tensor<T>,
    x: &Tensor<T>,
    w: &Tensor<T>,
    epsilon: T,
) {
    let len = y.size();
    assert!(len == x.size());
    let shape = y.shape().clone();
    let y_data = unsafe { y.data_mut() };
    let x = x.data();
    let w = w.data();
    for i in 0..shape[0] {
        let mut sum = T::zero();
        for j in 0..shape[1] {
            sum = sum + x[i * shape[1] + j] * x[i * shape[1] + j];
        }
        let rms = (sum / T::from(shape[1]).unwrap() + epsilon).sqrt();
        for j in 0..shape[1] {
            let idx = i * shape[1] + j;
            y_data[idx] = w[j] * (x[idx] / rms);
        }
    }
}
```

### 3. matmul_transb

```rust
// C = beta * C + alpha * A @ B^T
// hint: You don't need to do an explicit transpose of B
pub fn matmul_transb<T: Copy + Default + Float>(
    c: &mut Tensor<T>,
    beta: T,
    a: &Tensor<T>,
    b: &Tensor<T>,
    alpha: T,
) {
    let (m, n) = (a.shape()[0], a.shape()[1]);
    let p = b.shape()[0];
    let c_data = unsafe { c.data_mut() };
    let a = a.data();
    let b = b.data();
    for i in 0..m {
        for j in 0..p {
            let mut sum = T::zero();
            for k in 0..n {
                sum = sum + a[i * n + k] * b[j * n + k];
            }
            c_data[i * p + j] = beta * c_data[i * p + j] + alpha * sum;
        }
    }
}
```

### 4. mlp

```rust
fn mlp<T: Copy + Default + Float + Sum>(
    residual: &mut Tensor<T>,
    hidden_states: &mut Tensor<T>,
    gate: &mut Tensor<T>,
    up: &mut Tensor<T>,
    w_up: &Tensor<T>,
    w_down: &Tensor<T>,
    w_gate: &Tensor<T>,
    rms_w: &Tensor<T>,
    eps: T,
) {
    OP::rms_norm(hidden_states, residual, rms_w, eps);
    OP::matmul_transb(gate, T::zero(), hidden_states, w_gate, T::one());
    OP::matmul_transb(up, T::zero(), hidden_states, w_up, T::one());
    OP::swiglu(up, gate);
    OP::matmul_transb(residual, T::one(), up, w_down, T::one());
}
```

### 5. 模型参数加载

```rust
pub trait FromBytes: Sized {
    fn from_bytes(bytes: &[u8]) -> Vec<Self>;
}

impl FromBytes for f32 {
    fn from_bytes(bytes: &[u8]) -> Vec<Self> {
        bytemuck::cast_slice(bytes).to_vec()
    }
}
impl FromBytes for f16 {
    fn from_bytes(bytes: &[u8]) -> Vec<Self> {
        let expected = std::mem::size_of::<f16>();
        bytes
            .chunks_exact(expected)
            .map(|chunk| f16::from_le_bytes([chunk[0], chunk[1]]))
            .collect()
    }
}
impl FromBytes for bf16 {
    fn from_bytes(bytes: &[u8]) -> Vec<Self> {
        let expected = std::mem::size_of::<bf16>();
        bytes
            .chunks_exact(expected)
            .map(|chunk| bf16::from_le_bytes([chunk[0], chunk[1]]))
            .collect()
    }
}

impl<T: Copy + Clone + Default + FromBytes> LLamaParams<T> {
    pub(crate) fn from_safetensors(safetensor: &SafeTensors, config: &LlamaConfigJson) -> Self {
        let get_tensor = |name: &str| -> Tensor<T> {
            match safetensor.tensor(name) {
                Ok(v) => {
                    let data: Vec<T> = T::from_bytes(v.data());

                    Tensor::new(data, &v.shape().to_vec())
                }
                Err(_) => {
                    panic!("Failed to load tensor: {}", name);
                }
            }
        };

        let layers = config.num_hidden_layers;
        let embedding_table = if config.tie_word_embeddings {
            get_tensor("lm_head.weight")
        } else {
            get_tensor("model.embed_tokens.weight")
        };
        Self {
            embedding_table: embedding_table,
            rms_att_w: (0..layers)
                .map(|i| get_tensor(&format!("model.layers.{i}.input_layernorm.weight")))
                .collect(),
            wq: (0..layers)
                .map(|i| get_tensor(&format!("model.layers.{i}.self_attn.q_proj.weight")))
                .collect(),
            wk: (0..layers)
                .map(|i| get_tensor(&format!("model.layers.{i}.self_attn.k_proj.weight")))
                .collect(),
            wv: (0..layers)
                .map(|i| get_tensor(&format!("model.layers.{i}.self_attn.v_proj.weight")))
                .collect(),
            wo: (0..layers)
                .map(|i| get_tensor(&format!("model.layers.{i}.self_attn.o_proj.weight")))
                .collect(),
            rms_ffn_w: (0..layers)
                .map(|i| get_tensor(&format!("model.layers.{i}.post_attention_layernorm.weight")))
                .collect(),
            w_up: (0..layers)
                .map(|i| get_tensor(&format!("model.layers.{i}.mlp.up_proj.weight")))
                .collect(),
            w_gate: (0..layers)
                .map(|i| get_tensor(&format!("model.layers.{i}.mlp.gate_proj.weight")))
                .collect(),
            w_down: (0..layers)
                .map(|i| get_tensor(&format!("model.layers.{i}.mlp.down_proj.weight")))
                .collect(),
            rms_out_w: get_tensor("model.norm.weight"),
            lm_head: get_tensor("lm_head.weight"),
        }
    }
}
```

## 二、项目阶段

### 1. self_attention

```rust
fn self_attention<T: Copy + Default + Float + Sum>(
    hidden_states: &mut Tensor<T>, // (seq, n_kv_h * n_groups * dqkv)
    att_scores: &mut Tensor<T>,    // (n_kv_h, n_groups, seq, total_seq)
    q: &Tensor<T>,                 // (seq, n_kv_h * n_groups * dqkv)
    k: &Tensor<T>,                 // (total_seq, n_kv_h * dqkv)
    v: &Tensor<T>,                 // (total_seq, n_kv_h * dqkv)
    n_kv_h: usize,
    n_groups: usize,
    seq_len: usize,
    total_seq_len: usize,
    dqkv: usize,
) {
    let scale = T::from(dqkv as f32).unwrap().sqrt();
    let q_data = q.data();
    let k_data = k.data();
    let v_data = v.data();
    let att_data = unsafe { att_scores.data_mut() };

    for kv_head in 0..n_kv_h {
        for group in 0..n_groups {
            for q_seq in 0..seq_len {
                for k_seq in 0..total_seq_len {
                    let mut dot_product = T::zero();
                    for d in 0..dqkv {
                        let q_idx = q_seq * (n_kv_h * n_groups * dqkv)
                            + (kv_head * n_groups + group) * dqkv
                            + d;
                        let k_idx = k_seq * (n_kv_h * dqkv) + kv_head * dqkv + d;
                        dot_product = dot_product + q_data[q_idx] * k_data[k_idx];
                    }
                    let score_idx = kv_head * (n_groups * seq_len * total_seq_len)
                        + group * (seq_len * total_seq_len)
                        + q_seq * total_seq_len
                        + k_seq;
                    att_data[score_idx] = dot_product / scale;
                }
            }
        }
    }

    OP::masked_softmax(att_scores);

    let att_data = att_scores.data();
    let hidden_data = unsafe { hidden_states.data_mut() };
    for i in 0..hidden_data.len() {
        hidden_data[i] = T::zero();
    }

    for kv_head in 0..n_kv_h {
        for group in 0..n_groups {
            for q_seq in 0..seq_len {
                for d in 0..dqkv {
                    let mut sum = T::zero();
                    for k_seq in 0..total_seq_len {
                        let score_idx = kv_head * (n_groups * seq_len * total_seq_len)
                            + group * (seq_len * total_seq_len)
                            + q_seq * total_seq_len
                            + k_seq;
                        let v_idx = k_seq * (n_kv_h * dqkv) + kv_head * dqkv + d;
                        sum = sum + att_data[score_idx] * v_data[v_idx];
                    }
                    let out_idx = q_seq * (n_kv_h * n_groups * dqkv)
                        + (kv_head * n_groups + group) * dqkv
                        + d;
                    hidden_data[out_idx] = sum;
                }
            }
        }
    }
}
```

### 2. 功能：文本生成

forward

```rust
pub fn forward(&self, input: &Tensor<u32>, cache: &mut KVCache<T>) -> Tensor<T> {
    let seq_len = input.size();
    let past_seq_len = cache.len();
    cache.increment(seq_len);
    let total_seq_len = past_seq_len + seq_len;
    let n_groups = self.n_q_h / self.n_kv_h;

    // Some pre-allocated buffers that will be reused
    let mut residual = Tensor::<T>::default(&vec![seq_len, self.d]);
    let mut hidden_states = Tensor::<T>::default(&vec![seq_len, self.d]);
    let mut q_buf = Tensor::<T>::default(&vec![seq_len, self.n_q_h * self.dqkv]);
    let mut att_scores =
        Tensor::<T>::default(&vec![self.n_kv_h, n_groups, seq_len, total_seq_len]);
    let mut gate_buf = Tensor::<T>::default(&vec![seq_len, self.di]);
    let mut up_buf = Tensor::<T>::default(&vec![seq_len, self.di]);

    // Computation Starts Here
    // Embedding lookup
    OP::gather(&mut residual, input, &self.params.embedding_table);

    for layer in 0..self.n_layers {
        OP::rms_norm(
            &mut hidden_states,
            &residual,
            &self.params.rms_att_w[layer],
            self.eps,
        );

        let q = (&mut q_buf).reshape(&vec![seq_len, self.n_q_h * self.dqkv]); // (seq, n_h * dqkv)
        let k = &mut cache.k_cache(layer, past_seq_len); // (seq, n_kv_h * dqkv)
        let v = &mut cache.v_cache(layer, past_seq_len); // (seq, n_kv_h * dqkv)
        OP::matmul_transb(
            q,
            T::zero(),
            &hidden_states,
            &self.params.wq[layer],
            T::one(),
        );
        OP::matmul_transb(
            k,
            T::zero(),
            &hidden_states,
            &self.params.wk[layer],
            T::one(),
        );
        OP::matmul_transb(
            v,
            T::zero(),
            &hidden_states,
            &self.params.wv[layer],
            T::one(),
        );
        OP::rope(
            q.reshape(&vec![seq_len, self.n_q_h, self.dqkv]),
            past_seq_len,
            self.rope_theta,
        );
        OP::rope(
            k.reshape(&vec![seq_len, self.n_kv_h, self.dqkv]),
            past_seq_len,
            self.rope_theta,
        );

        let full_k = &mut cache.k_cache(layer, 0); // (total_seq, n_kv_h * dqkv)
        let full_v = &mut cache.v_cache(layer, 0); // (total_seq, n_kv_h * dqkv)

        self_attention(
            &mut hidden_states,
            &mut att_scores,
            q,
            full_k,
            full_v,
            self.n_kv_h,
            n_groups,
            seq_len,
            total_seq_len,
            self.dqkv,
        );

        OP::matmul_transb(
            &mut residual,
            T::one(),
            &hidden_states,
            &self.params.wo[layer],
            T::one(),
        );

        mlp(
            &mut residual,
            &mut hidden_states,
            &mut gate_buf,
            &mut up_buf,
            &self.params.w_up[layer],
            &self.params.w_down[layer],
            &self.params.w_gate[layer],
            &self.params.rms_ffn_w[layer],
            self.eps,
        );
    }

    // No matter what seq_len, the output is always a 1D vector of length vocab,
    // which contains the probabilities for the next token.
    let mut logits = Tensor::<T>::default(&vec![1, self.vocab]);
    let mut hidden_states = hidden_states.slice((seq_len - 1) * self.d, &vec![1, self.d]);
    let residual = residual.slice((seq_len - 1) * self.d, &vec![self.d]);

    OP::rms_norm(
        &mut hidden_states,
        &residual,
        &self.params.rms_out_w,
        self.eps,
    );

    OP::matmul_transb(
        &mut logits,
        T::zero(),
        &hidden_states,
        &self.params.lm_head,
        T::one(),
    );

    logits
}
```

generate

```rust
pub fn generate(
    &self,
    token_ids: &[u32],
    max_len: usize,
    top_p: T,
    top_k: u32,
    temperature: T,
) -> Vec<u32> {
    let mut result = vec![self.bos_token_id];
    result.extend_from_slice(token_ids);
    let mut cache = self.new_cache();

    let input_tensor = Tensor::new(result.clone(), &vec![result.len()]);
    let logits = self.forward(&input_tensor, &mut cache);

    let mut next_token = OP::random_sample(&logits, top_p, top_k, temperature);
    result.push(next_token);

    while result.len() < max_len {
        let input_tensor = Tensor::new(vec![next_token], &vec![1]);
        let logits = self.forward(&input_tensor, &mut cache);

        next_token = OP::random_sample(&logits, top_p, top_k, temperature);

        if next_token == self.eos_token_id {
            break;
        }

        result.push(next_token);
    }

    result
}
```

生成效果如下

```
% cargo run --bin story
Once upon a timeOnce upon a time, a brave cat named Tom lived on a tree in his yard. One day, Tom saw a big pink tree with many branches that looked pretty. Tom wanted to do it. He jumped on the tree and began to eat it.
Tom saw his friend, a dog named Max. Max said, "I want to eat it, Tom!" Max thought about what Sam said. Max tried to rest, but he could not get the pit on the tree. Tom did not know how to eat the pile of of the prune.
The prune was cold and cool. Tom was still a tree to get the apples inside. Now, he could eat the apples with the apples. Tom took the apples with his mouth and took the apples to share.
Tom and Max were having fun together. They had so much fun with their new prunch. They had a great time together and had a great time. They both played together and had a great time.<|end_story|>
```



### 3. AI对话

管理对话模板、会话对应kvcache的结构

```rust
#[derive(Clone)]
pub struct Message {
    role: String,
    content: String,
}

#[derive(Clone)]
pub struct ChatSession {
    messages: Vec<Message>,
}

impl ChatSession {
    pub fn new() -> Self {
        ChatSession {
            messages: Vec::new(),
        }
    }

    pub fn add_message(&mut self, role: &str, content: &str) {
        self.messages.push(Message {
            role: role.to_string(),
            content: content.to_string(),
        });
    }

    pub fn format_prompt(&self) -> String {
        let mut prompt = String::new();
        for msg in &self.messages {
            prompt.push_str(&format!(
                "<|im_start|>{}\n{}<|im_end|>\n",
                msg.role, msg.content
            ));
        }
        prompt.push_str("<|im_start|>assistant\n");
        prompt
    }
}
```

ai对话特化generate

```rust
pub fn chat(
    &self,
    promps: &str,
    cache: &mut KVCache<T>,
    tokenizer: &Tokenizer,
    max_len: usize,
    top_p: T,
    top_k: u32,
    temperature: T,
) -> String {
    let encoding = tokenizer.encode(promps, false).unwrap();
    let token_ids = encoding.get_ids();

    let input_tensor = Tensor::new(token_ids.to_vec(), &vec![token_ids.len()]);
    let logits = self.forward(&input_tensor, cache);

    let mut next_token = OP::random_sample(&logits, top_p, top_k, temperature);
    let mut result = vec![next_token];

    while result.len() < max_len {
        let input_tensor = Tensor::new(vec![next_token], &vec![1]);
        let logits = self.forward(&input_tensor, cache);

        next_token = OP::random_sample(&logits, top_p, top_k, temperature);

        if next_token == self.eos_token_id {
            break;
        }

        result.push(next_token);
    }

    tokenizer.decode(&result, true).unwrap()
}
```

ai对话对应main函数部分

```rust
use learning_lm_rust::chat;
#[cfg(not(feature = "cuda"))]
use learning_lm_rust::model::{self as model};
#[cfg(feature = "cuda")]
use learning_lm_rust::model_cuda::{self as model};
use std::io::{self, Write};
use std::path::PathBuf;
use tokenizers::Tokenizer;

fn main() {
    let project_dir = env!("CARGO_MANIFEST_DIR");
    let model_dir = PathBuf::from(project_dir).join("models").join("chat");
    let llama = model::Llama::<f32>::from_safetensors(&model_dir);
    let tokenizer = Tokenizer::from_file(model_dir.join("tokenizer.json")).unwrap();
    let mut cache = llama.new_cache();
    let mut session = chat::ChatSession::new();
    println!("欢迎使用聊天助手！输入 'quit' 结束对话。");

    session.add_message("system", "You are a helpful assistant");

    loop {
        print!("> ");
        io::stdout().flush().unwrap();

        let mut input = String::new();
        io::stdin().read_line(&mut input).unwrap();
        let input = input.trim();

        if input == "quit" {
            break;
        }

        session.add_message("user", input);

        let prompt = session.format_prompt();
        let response = llama.chat(&prompt, &mut cache, &tokenizer, 500, 0.8, 30, 1.);

        println!("Asistant: {}", response);
        session.add_message("assistant", &response);
    }
}

#[cfg(test)]
mod test {
    use std::path::PathBuf;

    use tokenizers::Tokenizer;

    #[test]
    fn test_tokenizer() -> Result<(), Box<dyn std::error::Error>> {
        let project_dir = env!("CARGO_MANIFEST_DIR");
        let model_dir = PathBuf::from(project_dir).join("models").join("chat");
        let tokenizer = Tokenizer::from_file(model_dir.join("tokenizer.json")).unwrap();

        let system_message = "你是一个有用的助手。";
        let user_message = "你好，今天天气怎么样？";

        let chat_input = format!(
            "<|im_start|>system\n{}<|im_end|>\n<|im_start|>user\n{}<|im_end|>\n<|im_start|>assistant",
            system_message, user_message
        );

        let encoding = tokenizer.encode(chat_input, false).unwrap();
        let token_ids = encoding.get_ids();

        assert_eq!(
            token_ids.to_vec(),
            vec![
                32001, 1587, 13, 29383, 28971, 28518, 28998, 28963, 28914, 30278, 29427, 28944,
                32000, 28705, 13, 32001, 2188, 13, 29383, 29530, 28924, 30316, 29354, 29354, 30627,
                31401, 29797, 29675, 29771, 32000, 28705, 13, 32001, 13892
            ]
        );
        Ok(())
    }
}
```



生成效果如下

```
% cargo run --bin chat
欢迎使用聊天助手！输入 'quit' 结束对话。
> Hello, how are you?
Asistant: I don't have any personal opinion about how you have been a part of my business, but here are some suggestions for how you can create a brand that reflects your company's culture:

1. Use your brand logo and tag team:

- choose a specific brand that is specific to your business or industry. - choose a brand that represents your company's culture and values. - use your brand to create a brand that resonates with your target audience. - use your brand logo and tag team to create a brand that is unique and engaging. - use your brand logo to build your brand's brand image and showcase your brand's brand as a standalone brand. - use your brand logo to create a brand logo and tag team, so you can stand out from the crowd. - use your brand logo to create a logo and tag team to create a brand logo. This will help to create a strong presence and increase brand recognition.
> 
```



### 4. cuda backend

注意力

```cpp
#include <cuda_bf16.h>
#include <cuda_fp16.h>

template<typename T>
__device__ T zero() {
    return T(0.0f);
}

template<typename T>
__device__ void attention_scores_kernel(
    T* scores,           // [n_kv_h, n_groups, seq_len, total_seq_len]
    const T* q,          // [seq_len, n_kv_h * n_groups * dqkv] 
    const T* k,          // [total_seq_len, n_kv_h * dqkv]
    const T scale,       // scalar
    const int seq_len,
    const int total_seq_len,
    const int n_kv_h,
    const int n_groups,
    const int dqkv
) {
    const int kv_head = blockIdx.x;
    const int group = blockIdx.y;
    const int q_seq = blockIdx.z;
    const int k_seq = threadIdx.x;

    if (kv_head >= n_kv_h || group >= n_groups || 
        q_seq >= seq_len || k_seq >= total_seq_len) {
        return;
    }

    T dot_product = zero<T>();
    for (int d = 0; d < dqkv; d++) {
        const int q_idx = q_seq * (n_kv_h * n_groups * dqkv) 
            + (kv_head * n_groups + group) * dqkv + d;
        const int k_idx = k_seq * (n_kv_h * dqkv) + kv_head * dqkv + d;
        dot_product += q[q_idx] * k[k_idx];
    }

    const int score_idx = kv_head * (n_groups * seq_len * total_seq_len)
        + group * (seq_len * total_seq_len)
        + q_seq * total_seq_len
        + k_seq;
    scores[score_idx] = dot_product * scale;
}

template<typename T>
__device__ void attention_output_kernel(
    T* output,           // [seq_len, n_kv_h * n_groups * dqkv]
    const T* scores,     // [n_kv_h, n_groups, seq_len, total_seq_len]
    const T* v,          // [total_seq_len, n_kv_h * dqkv]
    const int seq_len,
    const int total_seq_len, 
    const int n_kv_h,
    const int n_groups,
    const int dqkv
) {
    const int kv_head = blockIdx.x;
    const int group = blockIdx.y;
    const int q_seq = blockIdx.z;
    const int d = threadIdx.x;

    if (kv_head >= n_kv_h || group >= n_groups || 
        q_seq >= seq_len || d >= dqkv) {
        return;
    }

    T sum = zero<T>();
    for (int k_seq = 0; k_seq < total_seq_len; k_seq++) {
        const int score_idx = kv_head * (n_groups * seq_len * total_seq_len)
            + group * (seq_len * total_seq_len)
            + q_seq * total_seq_len
            + k_seq;
        const int v_idx = k_seq * (n_kv_h * dqkv) + kv_head * dqkv + d;
        sum += scores[score_idx] * v[v_idx];
    }

    const int out_idx = q_seq * (n_kv_h * n_groups * dqkv)
        + (kv_head * n_groups + group) * dqkv + d;
    output[out_idx] = sum;
}

#define OPS(TYPENAME, RUST_NAME) \
    extern "C" __global__ void attention_scores_##RUST_NAME( \
        TYPENAME* scores, \
        const TYPENAME* q, \
        const TYPENAME* k, \
        const TYPENAME scale, \
        const int seq_len, \
        const int total_seq_len, \
        const int n_kv_h, \
        const int n_groups, \
        const int dqkv \
    ) { \
        attention_scores_kernel<TYPENAME>( \
            scores, q, k, scale, seq_len, total_seq_len, \
            n_kv_h, n_groups, dqkv); \
    } \
    extern "C" __global__ void attention_output_##RUST_NAME( \
        TYPENAME* output, \
        const TYPENAME* scores, \
        const TYPENAME* v, \
        const int seq_len, \
        const int total_seq_len, \
        const int n_kv_h, \
        const int n_groups, \
        const int dqkv \
    ) { \
        attention_output_kernel<TYPENAME>( \
            output, scores, v, seq_len, total_seq_len, \
            n_kv_h, n_groups, dqkv); \
    }

#if __CUDA_ARCH__ >= 800
OPS(__nv_bfloat16, bf16)
#endif

#if __CUDA_ARCH__ >= 530
OPS(__half, f16)
#endif

OPS(float, f32)
OPS(double, f64)
```



dot

```cpp
#include "cuda_fp16.h"
#include "cuda_bf16.h"
#include<stdint.h>


const int BLOCK_SIZE = 1024;

template <typename T>
__device__ void dot(
    const T* x,
    const T* y,
    T* result,
    int n
) {
    __shared__ T shared[BLOCK_SIZE];

    int tid = threadIdx.x;
    int gid = blockIdx.x * blockDim.x + tid;

    T val = 0.0f;
    if (gid < n) {
        val = x[gid] * y[gid];
    }

    shared[tid] = val;
    __syncthreads();

    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            shared[tid] += shared[tid + stride];
        }
        __syncthreads();
    }

    if (tid == 0) {
        atomicAdd(result, shared[0]);
    }
}

#define OPS(TYPENAME, RUST_NAME) \
    extern "C" __global__ void dot_##RUST_NAME( \
        const TYPENAME* x, \
        const TYPENAME* y, \
        TYPENAME* result, \
        int n \
    ) { \
        dot<TYPENAME>(x, y, result, n); \
    } \

#if __CUDA_ARCH__ >= 800
OPS(__nv_bfloat16, bf16)
#endif

#if __CUDA_ARCH__ >= 530
OPS(__half, f16)
#endif

OPS(float, f32)
OPS(double, f64)
```



gather

```cpp
#include "cuda_fp16.h"
#include "cuda_bf16.h"
#include<stdint.h>

template <typename T>
__device__ void gather(
    T* output,
    const unsigned int* indices,
    const T* table,
    int length,
    int dim
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int dim_idx = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (idx < length && dim_idx < dim) {
        unsigned int table_idx = indices[idx];
        output[idx * dim + dim_idx] = table[table_idx * dim + dim_idx];
    }
}

#define OPS(TYPENAME, RUST_NAME) \
    extern "C" __global__ void gather_##RUST_NAME( \
        TYPENAME* output, \
        const unsigned int* indices, \
        const TYPENAME* table, \
        int length, \
        int dim \
    ) {\
    gather<TYPENAME>(output, indices, table, length, dim); \
    } \


#if __CUDA_ARCH__ >= 800
OPS(__nv_bfloat16, bf16)
#endif

#if __CUDA_ARCH__ >= 530
OPS(__half, f16)
#endif

OPS(float, f32)
OPS(double, f64)
```



matmul

```cpp
#include "cuda_fp16.h"
#include "cuda_bf16.h"
#include<stdint.h>

template <typename T>
__device__ void matmul_transb(
    T* C,
    T beta,
    T* A,
    T* B,
    T alpha,
    int M,
    int N,
    int P
) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < P) {
        T sum = T(0.0f);
        for (int k = 0; k < N; k++) {
            sum += A[row * N + k] * B[col * N + k];
        }
        C[row * P + col] = beta * C[row * P + col] + alpha * sum;
    }
}

#define OPS(TYPENAME, RUST_NAME) \
    extern "C" __global__ void matmul_transb_##RUST_NAME( \
        TYPENAME* C, \
        TYPENAME beta, \
        TYPENAME* A, \
        TYPENAME* B, \
        TYPENAME alpha, \
        int M, \
        int N, \
        int P \
    ) { \
        matmul_transb<TYPENAME>(C, beta, A, B, alpha, M, N, P); \
    } \

#if __CUDA_ARCH__ >= 800
OPS(__nv_bfloat16, bf16)
#endif

#if __CUDA_ARCH__ >= 530
OPS(__half, f16)
#endif

OPS(float, f32)
OPS(double, f64)
```



```cpp
#include "cuda_fp16.h"
#include "cuda_bf16.h"
#include<stdint.h>

using bf16 =  __nv_bfloat16;
using f16 = __half;

const int BLOCK_SIZE = 1024;

template<typename T>
__device__ void rms_norm(
    T* y,
    const T* x,
    const T* w,
    T epsilon,
    int m,
    int n
) {
    __shared__ T shared[BLOCK_SIZE];

    int i = blockIdx.x;
    if (i >= m) return;

    int tid = threadIdx.x;
    int stride = blockDim.x;

    T sum_val = 0.0f;
    for (int j = tid; j < n; j += stride) {
        T val = x[i * n + j];
        sum_val += val * val;
    }

    shared[tid] = sum_val;
    __syncthreads();

    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared[tid] += shared[tid + s];
        }
        __syncthreads();
    }

    T sum_squares = shared[0];
    T rms = sqrtf(sum_squares / T(n) + epsilon);

    for (int j = tid; j < n; j += stride) {
        T normalized = x[i * n + j] / rms;
        y[i * n + j] = normalized * w[j];
    }
}

#define OPS(TYPENAME, RUST_NAME) \
    extern "C" __global__ void rms_norm_##RUST_NAME( \
        TYPENAME* y, \
        const TYPENAME* x, \
        const TYPENAME* w, \
        TYPENAME epsilon, \
        int m, \
        int n \
    ) { \
        rms_norm<TYPENAME>(y, x, w, epsilon, m, n); \
    } \


#if __CUDA_ARCH__ >= 800
OPS(__nv_bfloat16, bf16)
#endif

#if __CUDA_ARCH__ >= 530
OPS(__half, f16)
#endif

OPS(float, f32)
OPS(double, f64)
```



```cpp
#include "cuda_fp16.h"
#include "cuda_bf16.h"
#include<stdint.h>

template <typename T>
__device__ void rope(
    T* data,
    int start_pos,
    float theta,
    int seq_len,
    int n_heads,
    int d
) {
    int tok = blockIdx.x * blockDim.x + threadIdx.x;
    int head = blockIdx.y * blockDim.y + threadIdx.y;
    int i = blockIdx.z * blockDim.z + threadIdx.z;

    if (tok < seq_len && head < n_heads && i < d/2) {
        int pos = start_pos + tok;
        int base_idx = tok * n_heads * d + head * d;
        
        T a = data[base_idx + i];
        T b = data[base_idx + i + d/2];
        
        float freq = (float)pos / powf(theta, (2.0f * i) / (float)d);
        float sin_val, cos_val;
        sincosf(freq, &sin_val, &cos_val);
        
        data[base_idx + i] = a * T(cos_val) - b * T(sin_val);
        data[base_idx + i + d/2] = b * T(cos_val) + a * T(sin_val);
    }
}

#define OPS(TYPENAME, RUST_NAME) \
    extern "C" __global__ void rope_##RUST_NAME( \
        TYPENAME* data, \
        int start_pos, \
        float theta, \
        int seq_len, \
        int n_heads, \
        int d \
    ) { \
        rope<TYPENAME>(data, start_pos, theta, seq_len, n_heads, d); \
    } \


#if __CUDA_ARCH__ >= 800
OPS(__nv_bfloat16, bf16)
#endif

#if __CUDA_ARCH__ >= 530
OPS(__half, f16)
#endif

OPS(float, f32)
OPS(double, f64)
```



random_sample

```cpp
#include <cuda_runtime.h>
#include <curand_kernel.h>
#include "cuda_fp16.h"
#include "cuda_bf16.h"
#include<stdint.h>

template<typename T>
__device__ T maximum(T a, T b) {
    return (a > b) ? a : b;
}

template<typename T>
__device__ T exp_wrapper(T x) {
    return expf(x);
}

template<>
__device__ __half exp_wrapper(__half x) {
    return hexp(x);
}

template<>
__device__ __nv_bfloat16 exp_wrapper(__nv_bfloat16 x) {
    return hexp(x);
}

template <typename T>
__device__ void random_sample(
    T* logits,        
    T* probs,         
    unsigned int* indices, 
    unsigned int* result,   
    float top_p,           
    unsigned int top_k,
    float temperature,
    unsigned int seed,
    int size
) {
    __shared__ T max_val;
    T local_max = -INFINITY;
    
    for(int i = threadIdx.x; i < size; i += blockDim.x) {
        local_max = maximum(local_max, logits[i]);
    }
    
    __shared__ T temp_max[256];
    temp_max[threadIdx.x] = local_max;
    __syncthreads();
    
    for(int stride = blockDim.x/2; stride > 0; stride >>= 1) {
        if(threadIdx.x < stride) {
            temp_max[threadIdx.x] = maximum(temp_max[threadIdx.x], temp_max[threadIdx.x + stride]);
        }
        __syncthreads();
    }
    
    if(threadIdx.x == 0) {
        max_val = temp_max[0];
    }
    __syncthreads();

    // softmax
    __shared__ T sum_exp;
    if(threadIdx.x == 0) {
        sum_exp = T(0.0f);
    }
    __syncthreads();

    for(int i = threadIdx.x; i < size; i += blockDim.x) {
        T val = exp_wrapper((logits[i] - max_val) / T(temperature));
        probs[i] = val;
        atomicAdd(&sum_exp, val);
    }
    __syncthreads();
    
    for(int i = threadIdx.x; i < size; i += blockDim.x) {
        probs[i] /= sum_exp;
        indices[i] = i;
    }
    __syncthreads();

    if(threadIdx.x == 0) {
        int k = min((int)top_k, size);
        for(int i = 0; i < k; i++) {
            int max_idx = i;
            for(int j = i + 1; j < size; j++) {
                if(probs[j] > probs[max_idx]) {
                    max_idx = j;
                }
            }
            if(max_idx != i) {
                T temp_prob = probs[i];
                probs[i] = probs[max_idx];
                probs[max_idx] = temp_prob;
                
                unsigned int temp_idx = indices[i];
                indices[i] = indices[max_idx];
                indices[max_idx] = temp_idx;
            }
        }

        T cumsum = T(0.0f);
        int last_idx = k - 1;
        if(top_p < 1.0f) {
            for(int i = 0; i < k; i++) {
                cumsum += probs[i];
                if(cumsum >= T(top_p)) {
                    last_idx = i;
                    break;
                }
            }
        }

        cumsum = T(0.0f);
        for(int i = 0; i <= last_idx; i++) {
            cumsum += probs[i];
        }
        for(int i = 0; i <= last_idx; i++) {
            probs[i] /= cumsum;
        }

        curandState state;
        curand_init(seed, 0, 0, &state);
        T rand_val = T(curand_uniform(&state));
        
        cumsum = T(0.0f);
        for(int i = 0; i <= last_idx; i++) {
            cumsum += probs[i];
            if(rand_val <= cumsum) {
                *result = indices[i];
                return;
            }
        }
        *result = indices[last_idx];
    }
}

#define OPS(TYPENAME, RUST_NAME) \
    extern "C" __global__ void random_sample_##RUST_NAME( \
        TYPENAME* logits, \
        TYPENAME* probs, \
        unsigned int* indices, \
        unsigned int* result, \
        float top_p, \
        unsigned int top_k, \
        float temperature, \
        unsigned int seed, \
        int size \
    ) { \
        random_sample<TYPENAME>(logits, probs, indices, result, top_p, top_k, temperature, seed, size); \
    }

#if __CUDA_ARCH__ >= 800
OPS(__nv_bfloat16, bf16)
#endif

#if __CUDA_ARCH__ >= 530
OPS(__half, f16)
#endif

OPS(float, f32)
OPS(double, f64)
```



softmax

```cpp
#include <cuda_bf16.h>
#include <cuda_fp16.h>

template<typename T>
__device__ T maximum(T a, T b) {
    return (a > b) ? a : b;
}

template<typename T>
__device__ T zero() {
    return T(0.0f);
}

template<typename T>
__device__ T exp_wrapper(T x) {
    return expf(x);
}

template<>
__device__ __half exp_wrapper(__half x) {
    return hexp(x);
}

template<>
__device__ __nv_bfloat16 exp_wrapper(__nv_bfloat16 x) {
    return hexp(x);
}

template <typename T>
__device__ void masked_softmax(
    T* data,
    const int batch,
    const int seq_len,
    const int total_seq_len
) {
    int b = blockIdx.x;
    int i = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (b < batch && i < seq_len) {
        int base = b * seq_len * total_seq_len;
        int offset = base + i * total_seq_len;
        int boundary = total_seq_len - seq_len + i + 1;
        
        T max_val = data[offset];
        for (int j = 1; j < boundary; j++) {
            max_val = maximum(max_val, data[offset + j]);
        }
        
        T sum = zero<T>();
        for (int j = 0; j < boundary; j++) {
            T val = exp_wrapper(data[offset + j] - max_val);
            data[offset + j] = val;
            sum += val;
        }
        
        for (int j = 0; j < boundary; j++) {
            data[offset + j] = data[offset + j] / sum;
        }
        for (int j = boundary; j < total_seq_len; j++) {
            data[offset + j] = zero<T>();
        }
    }
}

#define OPS(TYPENAME, RUST_NAME) \
    extern "C" __global__ void masked_softmax_##RUST_NAME( \
        TYPENAME* data, \
        const int batch, \
        const int seq_len, \
        const int total_seq_len \
    ) { \
        masked_softmax<TYPENAME>(data, batch, seq_len, total_seq_len); \
    } \

#if __CUDA_ARCH__ >= 800
OPS(__nv_bfloat16, bf16)
#endif

#if __CUDA_ARCH__ >= 530
OPS(__half, f16)
#endif

OPS(float, f32)
OPS(double, f64)
```



```cpp
#include "cuda_fp16.h"
#include "cuda_bf16.h"
#include<stdint.h>

template<typename T>
__device__ T exp_wrapper(T x) {
    return expf(x);
}

template<>
__device__ __half exp_wrapper(__half x) {
    return hexp(x);
}

template<>
__device__ __nv_bfloat16 exp_wrapper(__nv_bfloat16 x) {
    return hexp(x);
}

template <typename T>
__device__ void swiglu(
    T* y,
    const T* x,
    int num_elements
) {
    const int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i >= num_elements) return;

    const T x_val = x[i];
    const T sigmoid = T(1.0f) / (T(1.0f) + exp_wrapper(-x_val));
    const T silu = x_val * sigmoid;
    
    y[i] *= silu;
}


#define OPS(TYPENAME, RUST_NAME) \
    extern "C" __global__ void swiglu_##RUST_NAME( \
        TYPENAME* y, \
        const TYPENAME* x, \
        int num_elements \
    ) { \
        swiglu<TYPENAME>(y, x, num_elements); \
    } \


#if __CUDA_ARCH__ >= 800
OPS(__nv_bfloat16, bf16)
#endif

#if __CUDA_ARCH__ >= 530
OPS(__half, f16)
#endif

OPS(float, f32)
OPS(double, f64)
```



将cuda代码编译为ptx

`build.rs`

```rust
fn main() {
    println!("cargo:rerun-if-changed=build.rs");
    #[cfg(feature = "cuda")]
    {
        let builder = bindgen_cuda::Builder::default().kernel_paths_glob("cuda-kernels/**/*.cu");
        println!("cargo:info={builder:?}");
        let bindings = builder.build_ptx().unwrap();
        bindings
            .write("src/operators_cuda/cuda_kernels.rs")
            .unwrap();
    }
}
```



定义一些结构用于调用ptx

```rust
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum DType {
    F16,
    BF16,
    F32,
}

pub trait CudaDType: Sized + cudarc::driver::DeviceRepr {
    const DTYPE: DType;
    fn gemm(
        blas: &CudaBlas,
        cfg: GemmConfig<Self>,
        a: &CudaView<Self>,
        b: &CudaView<Self>,
        c: &mut CudaSlice<Self>,
    );
}

impl CudaDType for f32 {
    const DTYPE: DType = DType::F32;
    fn gemm(
        blas: &CudaBlas,
        cfg: GemmConfig<f32>,
        a: &CudaView<f32>,
        b: &CudaView<f32>,
        c: &mut CudaSlice<f32>,
    ) {
        unsafe {
            blas.gemm(cfg, a, b, c).unwrap();
        }
    }
}

impl CudaDType for f16 {
    const DTYPE: DType = DType::F16;
    fn gemm(
        blas: &CudaBlas,
        cfg: GemmConfig<f16>,
        a: &CudaView<f16>,
        b: &CudaView<f16>,
        c: &mut CudaSlice<f16>,
    ) {
        unsafe {
            blas.gemm(cfg, a, b, c).unwrap();
        }
    }
}

impl CudaDType for bf16 {
    const DTYPE: DType = DType::BF16;
    fn gemm(
        blas: &CudaBlas,
        cfg: GemmConfig<bf16>,
        a: &CudaView<bf16>,
        b: &CudaView<bf16>,
        c: &mut CudaSlice<bf16>,
    ) {
        unsafe {
            blas.gemm(cfg, a, b, c).unwrap();
        }
    }
}

#[derive(Clone)]
pub struct CudaOperator {
    dev: Arc<CudaDevice>,
    #[allow(unused)]
    blas: Arc<CudaBlas>,
}

fn kernel_name<T: CudaDType>(base_name: &str) -> String {
    let dtype_str = match T::DTYPE {
        DType::F16 => "f16",
        DType::BF16 => "bf16",
        DType::F32 => "f32",
    };
    format!("{base_name}_{dtype_str}")
}

impl CudaOperator {
    pub fn new() -> Self {
        let dev = CudaDevice::new(0).unwrap();
        let blas = Arc::new(CudaBlas::new(dev.clone()).unwrap());

        Self { dev, blas }
    }

    fn get_or_load_func(&self, module_name: &str, ptx: &'static str) -> CudaFunction {
        if !self.dev.has_func(module_name, module_name) {
            // Leaking the string here is a bit sad but we need a &'static str and this is only
            // done once per kernel name.
            let static_module_name = Box::leak(module_name.to_string().into_boxed_str());
            self.dev
                .load_ptx(ptx.into(), module_name, &[static_module_name])
                .unwrap();
        }
        let func = self.dev.get_func(module_name, module_name).unwrap();
        // Clippy recommends this `ok_or` rather than `ok_or_else` so hopefully the compiler is
        // able to only build the error value if needed.
        func
    }
```

matmal

```rust
// C = beta * C + alpha * A @ B^T
pub fn matmul_transb<T: CudaDType + Copy + Default>(
    &self,
    c: &mut Tensor<T>,
    beta: T,
    a: &Tensor<T>,
    b: &Tensor<T>,
    alpha: T,
) {
    let (m, n) = (a.shape()[0], a.shape()[1]);
    let p = b.shape()[0];
    let kname = kernel_name::<T>("matmul_transb");
    let f = self.get_or_load_func(&kname, cuda_kernels::MATMUL);

    // Convert tensors to f32 slices
    let a_host: &[T] =
        unsafe { std::slice::from_raw_parts(a.data().as_ptr() as *const T, a.data().len()) };
    let b_host: &[T] =
        unsafe { std::slice::from_raw_parts(b.data().as_ptr() as *const T, b.data().len()) };
    let c_host: &mut [T] = unsafe {
        std::slice::from_raw_parts_mut(c.data_mut().as_mut_ptr() as *mut T, c.data().len())
    };

    // Copy data to device
    let a_dev = self.dev.htod_sync_copy(a_host).unwrap();
    let b_dev = self.dev.htod_sync_copy(b_host).unwrap();
    let mut c_dev = self.dev.htod_sync_copy(c_host).unwrap();

    // Configure kernel launch parameters
    let block_dim = (16, 16, 1);
    let grid_dim = (
        (p as u32 + block_dim.0 - 1) / block_dim.0,
        (m as u32 + block_dim.1 - 1) / block_dim.1,
        1,
    );

    let cfg = LaunchConfig {
        block_dim,
        grid_dim,
        shared_mem_bytes: 0,
    };

    // Launch kernel
    unsafe {
        f.launch(
            cfg,
            (
                &mut c_dev, beta, &a_dev, &b_dev, alpha, m as i32, n as i32, p as i32,
            ),
        )
        .unwrap();
    }

    // Copy result back to host
    self.dev.dtoh_sync_copy_into(&c_dev, c_host).unwrap();
    self.dev.synchronize().unwrap();
}
```



rms_norm

```rust
pub fn rms_norm<T: CudaDType + Copy + Default + Float>(
    &self,
    y: &mut Tensor<T>,
    x: &Tensor<T>,
    w: &Tensor<T>,
    epsilon: T,
) {
    //assert_eq!(y.shape(), x.shape());
    let x_shape = x.shape();
    let (m, n) = match x_shape.len() {
        1 => (1, x_shape[0]),   
        2 => (x_shape[0], x_shape[1]),
        _ => panic!("Unsupported tensor shape"),
    };

    let x_host = x.data();
    let w_host = w.data();
    let y_host: &mut [T] = unsafe { y.data_mut() };
    let x_dev = self.dev.htod_sync_copy(x_host).unwrap();
    let w_dev = self.dev.htod_sync_copy(w_host).unwrap();
    let mut y_dev = self.dev.htod_sync_copy(y_host).unwrap();

    let kname = kernel_name::<T>("rms_norm");
    let func = self.get_or_load_func(&kname, cuda_kernels::RMS_NORM);

    let block_dim = 256;
    let grid_dim = m as u32;

    let shared_mem_bytes = block_dim * std::mem::size_of::<T>() as u32;

    let cfg = LaunchConfig {
        block_dim: (block_dim, 1, 1),
        grid_dim: (grid_dim, 1, 1),
        shared_mem_bytes,
    };

    let params = (&mut y_dev, &x_dev, &w_dev, epsilon, m as i32, n as i32);
    unsafe { func.launch(cfg, params).unwrap() };

    self.dev.dtoh_sync_copy_into(&y_dev, y_host).unwrap();
}
```

激活函数

```rust
// y = silu(x) * y
// hint: this is an element-wise operation
pub fn swiglu<T: Copy + Default + Float + Sum + CudaDType>(
    &self,
    y: &mut Tensor<T>,
    x: &Tensor<T>,
) {
    assert_eq!(y.size(), x.size());
    let num_elements = y.size();
    let kname = kernel_name::<T>("swiglu");
    let f = self.get_or_load_func(&kname, cuda_kernels::SWIGLU);

    let x_host: &[T] = x.data();
    let y_host: &mut [T] = unsafe { y.data_mut() };

    let x_dev = self.dev.htod_sync_copy(x_host).unwrap();
    let mut y_dev = self.dev.htod_sync_copy(y_host).unwrap();

    let block_dim = 256;
    let grid_dim = (num_elements as u32 + block_dim - 1) / block_dim;

    let cfg = LaunchConfig {
        block_dim: (block_dim, 1, 1),
        grid_dim: (grid_dim, 1, 1),
        shared_mem_bytes: 0,
    };

    unsafe {
        f.launch(cfg, (&mut y_dev, &x_dev, num_elements as i32))
            .unwrap();
    }

    self.dev.dtoh_sync_copy_into(&y_dev, y_host).unwrap();
}
```



rope

```rust
pub fn rope<T: Copy + Default + Float + CudaDType>(
    &self,
    y: &mut Tensor<T>,
    start_pos: usize,
    theta: T,
) {
    let shape = y.shape();
    assert!(shape.len() == 3);
    let seq_len = shape[0];
    let n_heads = shape[1];
    let d = shape[2];

    let kname = kernel_name::<T>("rope");
    let f = self.get_or_load_func(&kname, cuda_kernels::ROPE);

    let data_host: &mut [T] = unsafe {
        std::slice::from_raw_parts_mut(y.data_mut().as_mut_ptr() as *mut T, y.data().len())
    };

    let mut data_dev = self.dev.htod_sync_copy(data_host).unwrap();

    let block_dim = (8, 8, 8);
    let grid_dim = (
        (seq_len as u32 + block_dim.0 - 1) / block_dim.0,
        (n_heads as u32 + block_dim.1 - 1) / block_dim.1,
        (d as u32 / 2 + block_dim.2 - 1) / block_dim.2,
    );

    let cfg = LaunchConfig {
        block_dim,
        grid_dim,
        shared_mem_bytes: 0,
    };

    unsafe {
        f.launch(
            cfg,
            (
                &mut data_dev,
                start_pos as i32,
                theta.to_f32().unwrap(),
                seq_len as i32,
                n_heads as i32,
                d as i32,
            ),
        )
        .unwrap();
    }

    self.dev.dtoh_sync_copy_into(&data_dev, data_host).unwrap();
}
```



gather

```rust
pub fn gather<T: Copy + Default + CudaDType>(
    &self,
    y: &mut Tensor<T>,
    indices: &Tensor<u32>,
    table: &Tensor<T>,
) {
    let length = indices.size();
    let table_shape = table.shape();
    assert!(table_shape.len() == 2);
    let dim = table_shape[1];
    assert!(y.size() == length * dim);

    let kname = kernel_name::<T>("gather");
    let f = self.get_or_load_func(&kname, cuda_kernels::GATHER);

    let indices_host: &[u32] = indices.data();
    let table_host = unsafe {
        std::slice::from_raw_parts(table.data().as_ptr() as *const T, table.data().len())
    };
    let y_host = unsafe {
        std::slice::from_raw_parts_mut(y.data_mut().as_mut_ptr() as *mut T, y.data().len())
    };

    let indices_dev = self.dev.htod_sync_copy(indices_host).unwrap();
    let table_dev = self.dev.htod_sync_copy(table_host).unwrap();
    let mut y_dev = self.dev.htod_sync_copy(y_host).unwrap();

    let block_dim = (16, 16, 1);
    let grid_dim = (
        (length as u32 + block_dim.0 - 1) / block_dim.0,
        (dim as u32 + block_dim.1 - 1) / block_dim.1,
        1,
    );

    let cfg = LaunchConfig {
        block_dim,
        grid_dim,
        shared_mem_bytes: 0,
    };

    unsafe {
        f.launch(
            cfg,
            (
                &mut y_dev,
                &indices_dev,
                &table_dev,
                length as i32,
                dim as i32,
            ),
        )
        .unwrap();
    }

    self.dev.dtoh_sync_copy_into(&y_dev, y_host).unwrap();
}
```

softmax

```rust
// softmax(x) = exp(x - max) / sum(exp(x - max))
// y = softmax(mask(x))
pub fn masked_softmax<T: Copy + Default + Float + Sum + CudaDType>(&self, y: &mut Tensor<T>) {
    let ndim = y.shape().len();
    assert!(ndim >= 2);
    let seq_len = y.shape()[ndim - 2];
    let total_seq_len = y.shape()[ndim - 1];
    let batch = y.size() / (seq_len * total_seq_len);
    let kname = kernel_name::<T>("masked_softmax");
    let f = self.get_or_load_func(&kname, cuda_kernels::SOFTMAX);

    let data_host = unsafe {
        std::slice::from_raw_parts_mut(y.data_mut().as_mut_ptr() as *mut T, y.data().len())
    };
    let mut data_dev = self.dev.htod_sync_copy(data_host).unwrap();

    // Configure kernel launch parameters
    let block_dim = (1, 256, 1);
    let grid_dim = (
        batch as u32,
        (seq_len as u32 + block_dim.1 - 1) / block_dim.1,
        1,
    );

    let cfg = LaunchConfig {
        block_dim,
        grid_dim,
        shared_mem_bytes: 0,
    };

    unsafe {
        f.launch(
            cfg,
            (
                &mut data_dev,
                batch as i32,
                seq_len as i32,
                total_seq_len as i32,
            ),
        )
        .unwrap();
    }

    self.dev.dtoh_sync_copy_into(&data_dev, data_host).unwrap();
}
```

注意力

```rust
pub fn self_attention<T: Copy + Default + Float + Sum + CudaDType>(
    &self,
    hidden_states: &mut Tensor<T>, // (seq, n_kv_h * n_groups * dqkv)
    att_scores: &mut Tensor<T>,    // (n_kv_h, n_groups, seq, total_seq)
    q: &Tensor<T>,                 // (seq, n_kv_h * n_groups * dqkv)
    k: &Tensor<T>,                 // (total_seq, n_kv_h * dqkv)
    v: &Tensor<T>,                 // (total_seq, n_kv_h * dqkv)
    n_kv_h: usize,
    n_groups: usize,
    seq_len: usize,
    total_seq_len: usize,
    dqkv: usize,
) {
    let scale = T::from(dqkv as f32).unwrap().sqrt().recip();

    let q_host = q.data();
    let k_host = k.data();
    let v_host = v.data();
    let q_dev = self.dev.htod_sync_copy(q_host).unwrap();
    let k_dev = self.dev.htod_sync_copy(k_host).unwrap();
    let v_dev = self.dev.htod_sync_copy(v_host).unwrap();
    let mut scores_dev = self
        .dev
        .htod_sync_copy(unsafe { att_scores.data_mut() })
        .unwrap();
    let mut hidden_dev = self
        .dev
        .htod_sync_copy(unsafe { hidden_states.data_mut() })
        .unwrap();

    let scores_cfg = LaunchConfig {
        block_dim: (total_seq_len as u32, 1, 1),
        grid_dim: (n_kv_h as u32, n_groups as u32, seq_len as u32),
        shared_mem_bytes: 0,
    };

    let scores_kname = kernel_name::<T>("attention_scores");
    let scores_f = self.get_or_load_func(&scores_kname, cuda_kernels::ATTN);
    unsafe {
        scores_f
            .launch(
                scores_cfg,
                (
                    &mut scores_dev,
                    &q_dev,
                    &k_dev,
                    scale,
                    seq_len as i32,
                    total_seq_len as i32,
                    n_kv_h as i32,
                    n_groups as i32,
                    dqkv as i32,
                ),
            )
            .unwrap();
    }

    self.dev
        .dtoh_sync_copy_into(&scores_dev, unsafe { att_scores.data_mut() })
        .unwrap();
    self.masked_softmax(att_scores);
    scores_dev = self.dev.htod_sync_copy(att_scores.data()).unwrap();

    let output_cfg = LaunchConfig {
        block_dim: (dqkv as u32, 1, 1),
        grid_dim: (n_kv_h as u32, n_groups as u32, seq_len as u32),
        shared_mem_bytes: 0,
    };

    let output_kname = kernel_name::<T>("attention_output");
    let output_f = self.get_or_load_func(&output_kname, cuda_kernels::ATTN);
    unsafe {
        output_f
            .launch(
                output_cfg,
                (
                    &mut hidden_dev,
                    &scores_dev,
                    &v_dev,
                    seq_len as i32,
                    total_seq_len as i32,
                    n_kv_h as i32,
                    n_groups as i32,
                    dqkv as i32,
                ),
            )
            .unwrap();
    }

    self.dev
        .dtoh_sync_copy_into(&hidden_dev, unsafe { hidden_states.data_mut() })
        .unwrap();
}
```



在Llama定义中添加用于处理cuda的operator

```rust
pub struct Llama<T> {
    vocab: usize,           // vocab size
    n_layers: usize,        // number of layers
    n_q_h: usize,           // number of heads for q
    n_kv_h: usize,          // number of heads for k and v
    d: usize,               // dimension of hidden states
    dqkv: usize,            // length of a single q, k, or v vector
    di: usize,              // dimension of intermediate states
    eps: T,                 // epsilon for RMS normalization
    rope_theta: T,          // rope theta for rope initialization
    max_seq_len: usize,     // maximum sequence length
    params: LLamaParams<T>, // trained weights of this model
    bos_token_id: u32,      // start token id
    eos_token_id: u32,      // end token id
    pub operator: OP::CudaOperator,
}
```

文本生成效果如下

```
% cargo run --bin story --features="cuda"
Once upon a timeOnce upon a time, a dog named Spot lived in a small pool. He was eager to visit his friend Casty lived in a house with a fit who loved to measure it would look more powder and look for it.
One day, while Spot was walking, he met a new friend, a small cat named Fluffy, singing her mum, "Chould you like to pas?". Mrlly agreed. Spot nodded his head and said, "I like it." Spot was very happy and excited.
At night, Spot and Cassy went to well look at the pool. They all ran around, which they had ever before. After they finished, they saw many toys! A kind man had a picture of a happy song and a friend who was not worried anymore. They shared the new shape with the new muscles and made funny noises. They were happy they could help.<|end_story|>
```

ai对话效果如下

```
% cargo run --bin chat --features="cuda"
欢迎使用聊天助手！输入 'quit' 结束对话。
> Hello
Asistant: I am looking for a reliable and user-friendly website for searching for helpful and high-quality images. However, you can use it for search engine optimization and keyword research in various industries. You can also create a webpage that is easily accessible and easy to navigate for search engines.
> 
```





### 5. 混合精度

添加依赖

```toml
half = {version = "2.4.1", features = ["num-traits"]}
num-traits = "0.2.19"
```

将原来的函数改为了泛型函数



### 6. web api

sse的channel

```rust
use std::sync::Arc;
use std::{pin::Pin, task::Context, task::Poll, time::Duration};

use futures::Stream;
use ntex::web::{self, Error, HttpResponse};
use ntex::{time::interval, util::Bytes};
use parking_lot::Mutex;
use tokio::sync::mpsc::{channel, Receiver, Sender};

pub async fn new_client(broadcaster: web::types::State<Mutex<Broadcaster>>) -> HttpResponse {
    let rx = broadcaster.lock().new_client();

    HttpResponse::Ok()
        .header("content-type", "text/event-stream")
        .no_chunking()
        .streaming(rx)
}

pub struct Broadcaster {
    clients: Vec<Sender<Bytes>>,
}

impl Broadcaster {
    pub fn create() -> Arc<Mutex<Self>> {
        // Data ≃ Arc
        let me = Arc::new(Mutex::new(Broadcaster::new()));

        // ping clients every 10 seconds to see if they are alive
        Broadcaster::spawn_ping(me.clone());

        me
    }

    pub fn new() -> Self {
        Broadcaster {
            clients: Vec::new(),
        }
    }

    pub fn spawn_ping(me: Arc<Mutex<Self>>) {
        ntex::rt::spawn(async move {
            loop {
                let task = interval(Duration::from_secs(10));
                task.tick().await;
                me.lock().remove_stale_clients();
            }
        });
    }

    pub fn remove_stale_clients(&mut self) {
        let mut ok_clients = Vec::new();
        for client in self.clients.iter() {
            let result = client.clone().try_send(Bytes::from("data: ping\n\n"));

            if let Ok(()) = result {
                ok_clients.push(client.clone());
            }
        }
        self.clients = ok_clients;
    }

    pub fn new_client(&mut self) -> Client {
        let (tx, rx) = channel(100);

        tx.try_send(Bytes::from("data: connected\n\n")).unwrap();

        self.clients.push(tx);
        Client(rx)
    }

    pub fn send(&self, msg: &str) {
        let msg = Bytes::from(["data: ", msg, "\n\n"].concat());

        for client in self.clients.iter() {
            client.clone().try_send(msg.clone()).unwrap_or(());
        }
    }
}

// wrap Receiver in own type, with correct error type
pub struct Client(Receiver<Bytes>);

impl Stream for Client {
    type Item = Result<Bytes, Error>;

    fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        match Pin::new(&mut self.0).poll_recv(cx) {
            Poll::Ready(Some(v)) => Poll::Ready(Some(Ok(v))),
            Poll::Ready(None) => Poll::Ready(None),
            Poll::Pending => Poll::Pending,
        }
    }
}
```

web api特化generate

```rust
#[cfg(feature = "web")]
pub fn chat_stream<'a>(
    &'a self,
    promps: &str,
    cache: &'a mut KVCache<T>,
    tokenizer: &'a Tokenizer,
    max_len: usize,
    top_p: T,
    top_k: u32,
    temperature: T,
) -> impl Stream<Item = String> + 'a {
    use async_stream::stream;

    let encoding = tokenizer.encode(promps, false).unwrap();
    let token_ids = encoding.get_ids();

    let input_tensor = Tensor::new(token_ids.to_vec(), &vec![token_ids.len()]);
    let logits = self.forward(&input_tensor, cache);

    let mut next_token = OP::random_sample(&logits, top_p, top_k, temperature);
    let mut cnt = 1;
    stream! {
        let msg = tokenizer.decode(&vec![next_token], true).unwrap();

        yield msg;

        while cnt < max_len {
            let input_tensor = Tensor::new(vec![next_token], &vec![1]);
            let logits = self.forward(&input_tensor, cache);

            next_token = OP::random_sample(&logits, top_p, top_k, temperature);

            if next_token == self.eos_token_id {
                break;
            }

            yield tokenizer.decode(&vec![next_token], true).unwrap();
            cnt+=1;
        }
    }
}
```

使用ntex框架实现webapi

```rust
use futures::{pin_mut, StreamExt};
use learning_lm_rust::chat;
use learning_lm_rust::chat::ChatSession;
use learning_lm_rust::kvcache::KVCache;
#[cfg(not(feature = "cuda"))]
use learning_lm_rust::model::{self as model};
#[cfg(feature = "cuda")]
use learning_lm_rust::model_cuda::{self as model};
use learning_lm_rust::sse::Broadcaster;
use model::Llama;
use ntex::web::{self, HttpResponse};
use parking_lot::Mutex;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use tokenizers::Tokenizer;

#[derive(Clone)]
struct ChatManager {
    model: Arc<Llama<f32>>,
    tokenizer: Tokenizer,
    sessions: HashMap<String, (ChatSession, KVCache<f32>)>,
}

struct AppState {
    chat_manager: Arc<Mutex<ChatManager>>,
    broadcaster: Arc<Mutex<Broadcaster>>,
}

#[derive(Deserialize)]
struct ChatRequest {
    session_id: String,
    message: String,
}

#[derive(Serialize)]
struct ChatResponse {
    message: String,
}

async fn init_chat(
    session_id: web::types::Path<String>,
    data: web::types::State<AppState>,
    req: web::types::Json<ChatRequest>,
) -> HttpResponse {
    let mut manager = data.chat_manager.lock();

    if !manager.sessions.contains_key(&req.session_id) {
        let mut session = chat::ChatSession::new();
        session.add_message("system", "You are a helpful assistant");
        let cache = manager.model.new_cache();
        manager
            .sessions
            .insert(session_id.into_inner(), (session, cache));
    }

    let rx = data.broadcaster.lock().new_client();

    HttpResponse::Ok()
        .header("content-type", "text/event-stream")
        .no_chunking()
        .streaming(rx)
}

async fn send_message(
    data: web::types::State<AppState>,
    req: web::types::Json<ChatRequest>,
) -> HttpResponse {
    let mut manager = data.chat_manager.lock();
    let payload = manager.sessions.get_mut(&req.session_id);
    if let Some((session, cache)) = payload {
        session.add_message("user", &req.message);

        let prompt = session.format_prompt();
        let manager = data.chat_manager.lock();
        let token_stream =
            manager
                .model
                .chat_stream(&prompt, cache, &manager.tokenizer, 500, 0.8, 30, 1.);
        pin_mut!(token_stream);
        while let Some(token) = token_stream.next().await {
            let response = token.to_string();
            session.add_message("assistant", &response);
            data.broadcaster.lock().send(&response);

            session.add_message("assistant", &response);
        }

        HttpResponse::Ok().json(&ChatResponse {
            message: "msg sent".to_string(),
        })
    } else {
        HttpResponse::BadRequest().json(&ChatResponse {
            message: "Session not found".to_string(),
        })
    }
}

async fn index() -> HttpResponse {
    HttpResponse::Ok()
        .content_type("text/html")
        .body(include_str!("../static/index.html"))
}

#[ntex::main]
async fn main() -> std::io::Result<()> {
    let project_dir = env!("CARGO_MANIFEST_DIR");
    let model_dir = PathBuf::from(project_dir).join("models").join("chat");
    let model = model::Llama::<f32>::from_safetensors(&model_dir);
    let tokenizer = Tokenizer::from_file(model_dir.join("tokenizer.json")).unwrap();

    let chat_manager = ChatManager {
        model: Arc::new(model),
        tokenizer,
        sessions: HashMap::new(),
    };

    let broadcaster = Broadcaster::create();

    println!("Starting server at http://127.0.0.1:8080");

    web::HttpServer::new(move || {
        web::App::new()
            .state(AppState {
                chat_manager: Arc::new(Mutex::new(chat_manager.clone())),
                broadcaster: broadcaster.clone(),
            })
            .service(web::resource("/").to(index))
            .service(web::resource("/api/init/{session_id}").to(init_chat))
            .service(web::resource("/api/chat").to(send_message))
    })
    .bind(("127.0.0.1", 8080))?
    .run()
    .await
}
```

web前端

```html
<!DOCTYPE html>
<html>
<head>
    <title>AI Chat</title>
    <style>
        body {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            font-family: Arial, sans-serif;
        }
        #chat-box {
            height: 400px;
            border: 1px solid #ccc;
            margin-bottom: 20px;
            padding: 10px;
            overflow-y: auto;
        }
        .message {
            margin-bottom: 10px;
            padding: 8px;
            border-radius: 4px;
        }
        .user {
            background-color: #e3f2fd;
            margin-left: 20%;
        }
        .assistant {
            background-color: #f5f5f5;
            margin-right: 20%;
        }
        #input-box {
            width: 100%;
            display: flex;
            gap: 10px;
        }
        #message-input {
            flex-grow: 1;
            padding: 8px;
        }
        button {
            padding: 8px 16px;
        }
    </style>
</head>
<body>
    <h1>AI Chat</h1>
    <div id="chat-box"></div>
    <div id="input-box">
        <input type="text" id="message-input" placeholder="Type your message...">
        <button onclick="sendMessage()">Send</button>
    </div>

    <script>
        const sessionId = Math.random().toString(36).substring(7);
        let isInitialized = false;
        let events;
        let currentAssistantMessage = null;
        let isProcessing = false;
        let messageQueue = [];

        async function initChat() {
            console.log(sessionId);
            events = new EventSource(`/api/init/${sessionId}`);
            
            events.addEventListener('message', (event) => {
                const data = event.data;
                if (data === '[DONE]') {
                    currentAssistantMessage = null;
                    return;
                }
                
                if (!currentAssistantMessage) return;
                
                messageQueue.push(...data.split(''));
                if (!isProcessing) {
                    processMessageQueue();
                }
            });

            isInitialized = true;
        }

        function processMessageQueue() {
            if (messageQueue.length === 0 || !currentAssistantMessage) {
                isProcessing = false;
                return;
            }

            isProcessing = true;
            const char = messageQueue.shift();
            currentAssistantMessage.textContent += char;
            
            // 自动滚动到底部
            const chatBox = document.getElementById('chat-box');
            chatBox.scrollTop = chatBox.scrollHeight;

            setTimeout(processMessageQueue, 30);
        }

        async function sendMessage() {
            if (!isInitialized) {
                await initChat();
            }

            const input = document.getElementById('message-input');
            const message = input.value.trim();
            if (!message) return;

            // 显示用户消息
            addMessage('user', message);
            input.value = '';

            // 创建新的助手消息容器
            currentAssistantMessage = addMessage('assistant', '');
            messageQueue = [];
            isProcessing = false;

            // 发送到服务器
            await fetch('/api/chat', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                    session_id: sessionId,
                    message: message
                })
            });
        }

        function addMessage(role, content) {
            const chatBox = document.getElementById('chat-box');
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${role}`;
            messageDiv.textContent = content;
            chatBox.appendChild(messageDiv);
            chatBox.scrollTop = chatBox.scrollHeight;
            return messageDiv;
        }

        document.getElementById('message-input').addEventListener('keypress', function(e) {
            if (e.key === 'Enter') {
                sendMessage();
            }
        });

        initChat();
    </script>
</body>
</html>
```



后台服务启动后控制台输出如下

```
% cargo run --bin web --features="web"  
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.13s
     Running `target/debug/web`
Starting server at http://127.0.0.1:8080
```

